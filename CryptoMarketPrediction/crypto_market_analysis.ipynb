{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pearson Correlation Coefficient (PCC) for Model Evaluation\n",
        "\n",
        "The Pearson correlation coefficient (ρ) measures the linear correlation between two variables, in our case between the actual labels (y) and predicted values (ŷ) on the private test set. It ranges from -1 to +1, where:\n",
        "\n",
        "- +1 indicates perfect positive linear correlation\n",
        "- 0 indicates no linear correlation  \n",
        "- -1 indicates perfect negative linear correlation\n",
        "\n",
        "# The formula for PCC is:\n",
        "# \n",
        "# $\\rho = \\frac{Cov(y,\\hat{y})}{\\sigma_y \\cdot \\sigma_{\\hat{y}}}$\n",
        "\n",
        "Where:\n",
        "- Cov(y,ŷ) is the covariance between actual and predicted values\n",
        "- σy is the standard deviation of actual values\n",
        "- σŷ is the standard deviation of predicted values\n",
        "\n",
        "For this competition, the evaluation metric is the Pearson correlation between our predictions and the true labels on the private test set. A higher positive correlation indicates better predictive performance, as our predictions more closely track the actual market movements.\n",
        "\n",
        "Key implications:\n",
        "- We want our predictions to move in the same direction as actual values\n",
        "- The magnitude of movements matters less than getting the direction right\n",
        "- A correlation of 0.7+ would indicate strong predictive performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Ready to analyze DRW Crypto Market Prediction data...\n"
          ]
        }
      ],
      "source": [
        "# DRW - Crypto Market Prediction Analysis\n",
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold\n",
        "import os\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options for better output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to analyze DRW Crypto Market Prediction data...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Data:\n",
            "Shape: (525887, 896)\n",
            "\n",
            "First few rows (showing first 7 columns):\n",
            "                     bid_qty  ask_qty  buy_qty  sell_qty   volume        X1  \\\n",
            "timestamp                                                                     \n",
            "2023-03-01 00:00:00   15.283    8.425  176.405    44.984  221.389  0.121263   \n",
            "2023-03-01 00:01:00   38.590    2.336  525.846   321.950  847.796  0.302841   \n",
            "2023-03-01 00:02:00    0.442   60.250  159.227   136.369  295.596  0.167462   \n",
            "2023-03-01 00:03:00    4.865   21.016  335.742   124.963  460.705  0.072944   \n",
            "2023-03-01 00:04:00   27.158    3.451   98.411    44.407  142.818  0.173820   \n",
            "\n",
            "                           X2  \n",
            "timestamp                      \n",
            "2023-03-01 00:00:00 -0.417690  \n",
            "2023-03-01 00:01:00 -0.049576  \n",
            "2023-03-01 00:02:00 -0.291212  \n",
            "2023-03-01 00:03:00 -0.436590  \n",
            "2023-03-01 00:04:00 -0.213489  \n",
            "\n",
            "Test Data:\n",
            "Shape: (538150, 896)\n",
            "\n",
            "First few rows (showing first 7 columns):\n",
            "    bid_qty  ask_qty  buy_qty  sell_qty   volume        X1        X2\n",
            "ID                                                                  \n",
            "1     0.114   12.121   10.587    10.971   21.558 -0.732818  0.512331\n",
            "2     2.426    2.962  136.241    12.304  148.545 -0.337995 -0.412176\n",
            "3     1.085    2.343   23.390    57.171   80.561  0.111249  0.458221\n",
            "4    14.793    1.117  116.518    13.082  129.600 -0.149399 -0.640638\n",
            "5     0.033   14.178   43.800    49.836   93.636 -0.694662  0.611254\n",
            "\n",
            "Sample Submission:\n",
            "Shape: (538150, 2)\n",
            "\n",
            "First few rows:\n",
            "   ID  prediction\n",
            "0   1   -0.280233\n",
            "1   2    1.371969\n",
            "2   3   -2.045252\n",
            "3   4   -1.447555\n",
            "4   5   -1.303901\n"
          ]
        }
      ],
      "source": [
        "# Load and examine the crypto market data\n",
        "# First, let's identify the main data files (both CSV and Parquet)\n",
        "data_dir = \"./data/\"\n",
        "all_files = os.listdir(data_dir)\n",
        "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
        "parquet_files = [f for f in all_files if f.endswith('.parquet')]\n",
        "\n",
        "# Load the main dataset(s)\n",
        "train_data = None\n",
        "test_data = None\n",
        "sample_submission = None\n",
        "\n",
        "# Function to load data based on file extension\n",
        "def load_data_file(filepath):\n",
        "    if filepath.endswith('.csv'):\n",
        "        return pd.read_csv(filepath)\n",
        "    elif filepath.endswith('.parquet'):\n",
        "        return pd.read_parquet(filepath)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
        "\n",
        "# Try to identify train, test, and submission files\n",
        "all_data_files = csv_files + parquet_files\n",
        "for file in all_data_files:\n",
        "    filepath = os.path.join(data_dir, file)\n",
        "    \n",
        "    if 'train' in file.lower():\n",
        "        train_data = load_data_file(filepath)\n",
        "        \n",
        "    elif 'test' in file.lower() and 'submission' not in file.lower():\n",
        "        test_data = load_data_file(filepath)\n",
        "        \n",
        "    elif 'submission' in file.lower():\n",
        "        sample_submission = load_data_file(filepath)\n",
        "\n",
        "\n",
        "# Display basic information about the datasets\n",
        "if train_data is not None:\n",
        "    print(\"\\nTraining Data:\")\n",
        "    print(f\"Shape: {train_data.shape}\")\n",
        "    print(\"\\nFirst few rows (showing first 7 columns):\")\n",
        "    print(train_data.iloc[:5, :7])\n",
        "\n",
        "if test_data is not None:\n",
        "    print(\"\\nTest Data:\")\n",
        "    print(f\"Shape: {test_data.shape}\")\n",
        "    print(\"\\nFirst few rows (showing first 7 columns):\")\n",
        "    print(test_data.iloc[:5, :7])\n",
        "\n",
        "if sample_submission is not None:\n",
        "    print(\"\\nSample Submission:\")\n",
        "    print(f\"Shape: {sample_submission.shape}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(sample_submission.head())\n",
        "\n",
        "if train_data is None and test_data is None:\n",
        "    print(\"No train or test data loaded. Please check if the data files exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data successfully split into training and testing sets.\n",
            "X (training features) shape: (525887, 895)\n",
            "y (training target) shape: (525887,)\n",
            "X_test (test features) shape: (538150, 895)\n"
          ]
        }
      ],
      "source": [
        "target_col = 'label'\n",
        "\n",
        "if target_col in train_data.columns:\n",
        "    X = train_data.drop(target_col, axis=1)\n",
        "    y = train_data[target_col]\n",
        "\n",
        "    if target_col in test_data.columns:\n",
        "        X_test = test_data.drop(target_col, axis=1)\n",
        "    else:\n",
        "        X_test = test_data.copy() # Use .copy() to avoid SettingWithCopyWarning later\n",
        "\n",
        "    print(\"Data successfully split into training and testing sets.\")\n",
        "    print(f\"X (training features) shape: {X.shape}\")\n",
        "    print(f\"y (training target) shape: {y.shape}\")\n",
        "    print(f\"X_test (test features) shape: {X_test.shape}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Target column '{target_col}' not found in the training data.\")\n",
        "    print(\"Please update the 'target_col' variable with the correct column name.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature selection applied.\n",
            "New shape of X (training features): (525887, 25)\n",
            "New shape of X_test (test features): (538150, 25)\n"
          ]
        }
      ],
      "source": [
        "# Define the list of features identified as most important through prior analysis.\n",
        "# This combines domain-specific features and data-driven anonymous features.\n",
        "\n",
        "features = [\n",
        "    # Top features identified from the referenced Kaggle notebook\n",
        "    \"X863\", \"X856\", \"X344\", \"X598\", \"X862\", \"X385\", \"X852\", \"X603\", \"X860\", \"X674\",\n",
        "    \"X415\", \"X345\", \"X137\", \"X855\", \"X174\", \"X302\", \"X178\", \"X532\", \"X168\", \"X612\",\n",
        "    \n",
        "    # Core market microstructure features\n",
        "    \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\"\n",
        "]\n",
        "\n",
        "X = X[features]\n",
        "X_test = X_test[features]\n",
        "\n",
        "print(\"Feature selection applied.\")\n",
        "print(f\"New shape of X (training features): {X.shape}\")\n",
        "print(f\"New shape of X_test (test features): {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting XGBoost training with 5-fold cross-validation...\n",
            "--- Fold 1/5 ---\n",
            "Pearson Correlation for Fold 1: 0.67993\n",
            "--- Fold 2/5 ---\n",
            "Pearson Correlation for Fold 2: 0.68663\n",
            "--- Fold 3/5 ---\n",
            "Pearson Correlation for Fold 3: 0.66705\n",
            "--- Fold 4/5 ---\n",
            "Pearson Correlation for Fold 4: 0.67591\n",
            "--- Fold 5/5 ---\n",
            "Pearson Correlation for Fold 5: 0.66471\n",
            "\n",
            "--- Cross-Validation Summary ---\n",
            "Average Pearson Correlation: 0.67485\n",
            "Standard Deviation of Scores: 0.00812\n",
            "\n",
            "Submission file 'submission.csv' created successfully!\n",
            "First 5 rows of the submission file:\n",
            "   ID  prediction\n",
            "0   1    0.273182\n",
            "1   2    0.272615\n",
            "2   3    0.193447\n",
            "3   4   -0.107047\n",
            "4   5    0.337480\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# --- XGBoost Model Training with Cross-Validation ---\n",
        "\n",
        "# 1. Define Model Parameters\n",
        "# These are starting parameters. The best values are typically found through hyperparameter tuning.\n",
        "params = {\n",
        "    'n_estimators': 500,          # Number of boosting rounds.\n",
        "    'learning_rate': 0.05,        # Step size shrinkage to prevent overfitting.\n",
        "    'max_depth': 4,               # Maximum depth of a tree.\n",
        "    'subsample': 0.8,             # Fraction of samples to be used for fitting the individual base learners.\n",
        "    'colsample_bytree': 0.8,      # Fraction of columns to be used when constructing each tree.\n",
        "    'objective': 'reg:squarederror', # Specifies the learning task and objective.\n",
        "    'random_state': 42,           # Seed for reproducibility.\n",
        "    'n_jobs': -1,                 # Use all available CPU cores for training.\n",
        "    'eval_metric': 'rmse'         # **FIX**: Moved the metric here for early stopping evaluation.\n",
        "}\n",
        "\n",
        "# 2. Set up Cross-Validation\n",
        "N_SPLITS = 5\n",
        "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "\n",
        "# 3. Initialize storage for scores and predictions\n",
        "oof_scores = []\n",
        "test_predictions = np.zeros(len(X_test))\n",
        "\n",
        "print(\"Starting XGBoost training with 5-fold cross-validation...\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
