{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Jane Street Market Data Analysis\n",
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
        "import seaborn as sns\n",
        "import os\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options for better output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data directory if it doesn't exist\n",
        "data_dir = \"data\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "    print(f\"Created directory: {data_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {data_dir}\")\n",
        "\n",
        "# Download the data using Kaggle CLI\n",
        "# Note: Make sure you have kaggle CLI installed and configured with your API key\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    # Download the competition data\n",
        "    print(\"Downloading Jane Street Market Data...\")\n",
        "    result = subprocess.run(\n",
        "        [\"kaggle\", \"competitions\", \"download\", \"-c\", \"jane-street-real-time-market-data-forecasting\", \"-p\", data_dir],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"Data downloaded successfully!\")\n",
        "        print(f\"Files saved to: {data_dir}\")\n",
        "    else:\n",
        "        print(f\"Error downloading data: {result.stderr}\")\n",
        "        print(\"Please make sure you have:\")\n",
        "        print(\"1. Kaggle CLI installed (pip install kaggle)\")\n",
        "        print(\"2. API key configured (~/.kaggle/kaggle.json)\")\n",
        "        print(\"3. Accepted the competition rules\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please run the following command in terminal:\")\n",
        "    print(\"kaggle competitions download -c jane-street-real-time-market-data-forecasting -p data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the data from zip file\n",
        "zip_path = os.path.join(data_dir, \"jane-street-real-time-market-data-forecasting.zip\")\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    print(\"Extracting data from zip file...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    print(\"Data extracted successfully!\")\n",
        "else:\n",
        "    print(f\"Zip file not found at: {zip_path}\")\n",
        "    print(\"Please ensure the data has been downloaded first.\")\n",
        "\n",
        "# List all files in the data directory\n",
        "print(\"\\nFiles in data directory:\")\n",
        "if os.path.exists(data_dir):\n",
        "    for file in os.listdir(data_dir):\n",
        "        file_path = os.path.join(data_dir, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
        "            print(f\"  {file} ({size:.2f} MB)\")\n",
        "else:\n",
        "    print(\"Data directory not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and explore the data\n",
        "# First, let's identify the main data files\n",
        "\n",
        "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
        "print(f\"Found CSV files: {csv_files}\")\n",
        "\n",
        "# Load the main training data (assuming it's named something like train.csv or similar)\n",
        "train_data = None\n",
        "test_data = None\n",
        "\n",
        "for file in csv_files:\n",
        "    if 'train' in file.lower():\n",
        "        print(f\"\\nLoading training data from: {file}\")\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, file))\n",
        "        print(f\"Training data shape: {train_data.shape}\")\n",
        "        break\n",
        "\n",
        "# If no train file found, load the first CSV file\n",
        "if train_data is None and csv_files:\n",
        "    print(f\"\\nLoading first available CSV file: {csv_files[0]}\")\n",
        "    train_data = pd.read_csv(os.path.join(data_dir, csv_files[0]))\n",
        "    print(f\"Data shape: {train_data.shape}\")\n",
        "\n",
        "# Display basic information about the dataset\n",
        "if train_data is not None:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATASET OVERVIEW\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Dataset shape: {train_data.shape}\")\n",
        "    print(f\"Number of rows: {train_data.shape[0]:,}\")\n",
        "    print(f\"Number of columns: {train_data.shape[1]:,}\")\n",
        "    \n",
        "    print(\"\\nColumn names:\")\n",
        "    for i, col in enumerate(train_data.columns):\n",
        "        print(f\"  {i+1:2d}. {col}\")\n",
        "        \n",
        "    print(\"\\nData types:\")\n",
        "    print(train_data.dtypes.value_counts())\n",
        "    \n",
        "    print(\"\\nMemory usage:\")\n",
        "    print(f\"Total memory usage: {train_data.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "else:\n",
        "    print(\"No data loaded. Please check if the data files exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the head of the dataset\n",
        "if train_data is not None:\n",
        "    print(\"=\"*50)\n",
        "    print(\"DATA HEAD (First 5 rows)\")\n",
        "    print(\"=\"*50)\n",
        "    display(train_data.head())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATA TAIL (Last 5 rows)\")\n",
        "    print(\"=\"*50)\n",
        "    display(train_data.tail())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"BASIC STATISTICS\")\n",
        "    print(\"=\"*50)\n",
        "    display(train_data.describe())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MISSING VALUES\")\n",
        "    print(\"=\"*50)\n",
        "    missing_values = train_data.isnull().sum()\n",
        "    missing_percent = (missing_values / len(train_data)) * 100\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Column': missing_values.index,\n",
        "        'Missing_Count': missing_values.values,\n",
        "        'Missing_Percent': missing_percent.values\n",
        "    })\n",
        "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "    \n",
        "    if not missing_df.empty:\n",
        "        display(missing_df)\n",
        "    else:\n",
        "        print(\"No missing values found!\")\n",
        "        \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SAMPLE OF RANDOM ROWS\")\n",
        "    print(\"=\"*50)\n",
        "    display(train_data.sample(5))\n",
        "else:\n",
        "    print(\"No data to display.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation Analysis\n",
        "if train_data is not None:\n",
        "    print(\"=\"*50)\n",
        "    print(\"CORRELATION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Select only numeric columns for correlation\n",
        "    numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
        "    print(f\"Found {len(numeric_cols)} numeric columns for correlation analysis\")\n",
        "    \n",
        "    if len(numeric_cols) > 1:\n",
        "        # Calculate correlation matrix\n",
        "        corr_matrix = train_data[numeric_cols].corr()\n",
        "        \n",
        "        # Display correlation matrix\n",
        "        print(\"\\nCorrelation Matrix (top 10x10):\")\n",
        "        display(corr_matrix.iloc[:10, :10])\n",
        "        \n",
        "        # Find highly correlated pairs\n",
        "        print(\"\\nHighly Correlated Features (|correlation| > 0.7):\")\n",
        "        high_corr_pairs = []\n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i+1, len(corr_matrix.columns)):\n",
        "                corr_val = corr_matrix.iloc[i, j]\n",
        "                if abs(corr_val) > 0.7:\n",
        "                    high_corr_pairs.append((\n",
        "                        corr_matrix.columns[i], \n",
        "                        corr_matrix.columns[j], \n",
        "                        corr_val\n",
        "                    ))\n",
        "        \n",
        "        if high_corr_pairs:\n",
        "            high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Feature1', 'Feature2', 'Correlation'])\n",
        "            high_corr_df = high_corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
        "            display(high_corr_df.head(20))\n",
        "        else:\n",
        "            print(\"No highly correlated features found (|correlation| > 0.7)\")\n",
        "            \n",
        "        # Create correlation heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        \n",
        "        # If too many features, show only the first 20x20\n",
        "        if len(numeric_cols) > 20:\n",
        "            corr_subset = corr_matrix.iloc[:20, :20]\n",
        "            title = \"Correlation Heatmap (First 20 features)\"\n",
        "        else:\n",
        "            corr_subset = corr_matrix\n",
        "            title = \"Correlation Heatmap\"\n",
        "            \n",
        "        sns.heatmap(corr_subset, annot=True, cmap='coolwarm', center=0, \n",
        "                   square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Show correlation with target variable if exists\n",
        "        target_candidates = ['target', 'label', 'y', 'price', 'return']\n",
        "        target_col = None\n",
        "        for col in target_candidates:\n",
        "            if col in train_data.columns:\n",
        "                target_col = col\n",
        "                break\n",
        "                \n",
        "        if target_col:\n",
        "            print(f\"\\nCorrelation with target variable '{target_col}':\")\n",
        "            target_corr = train_data[numeric_cols].corrwith(train_data[target_col]).sort_values(key=abs, ascending=False)\n",
        "            display(target_corr.head(10))\n",
        "        else:\n",
        "            print(\"\\nNo obvious target variable found. Looking for potential targets...\")\n",
        "            # Show correlation with any column that might be a target\n",
        "            for col in numeric_cols:\n",
        "                if any(keyword in col.lower() for keyword in ['target', 'label', 'y', 'price', 'return']):\n",
        "                    print(f\"\\nCorrelation with '{col}':\")\n",
        "                    col_corr = train_data[numeric_cols].corrwith(train_data[col]).sort_values(key=abs, ascending=False)\n",
        "                    display(col_corr.head(10))\n",
        "                    break\n",
        "    else:\n",
        "        print(\"Not enough numeric columns for correlation analysis\")\n",
        "else:\n",
        "    print(\"No data available for correlation analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional Exploratory Data Analysis\n",
        "if train_data is not None:\n",
        "    print(\"=\"*50)\n",
        "    print(\"ADDITIONAL EXPLORATORY ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Distribution of numeric features\n",
        "    numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
        "    \n",
        "    if len(numeric_cols) > 0:\n",
        "        print(f\"\\nPlotting distributions for first 6 numeric features...\")\n",
        "        \n",
        "        # Create distribution plots\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        axes = axes.ravel()\n",
        "        \n",
        "        for i, col in enumerate(numeric_cols[:6]):\n",
        "            train_data[col].hist(bins=30, ax=axes[i], alpha=0.7)\n",
        "            axes[i].set_title(f'Distribution of {col}')\n",
        "            axes[i].set_xlabel(col)\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "        \n",
        "        # Hide empty subplots\n",
        "        for i in range(len(numeric_cols[:6]), 6):\n",
        "            axes[i].set_visible(False)\n",
        "            \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Check for categorical columns\n",
        "    categorical_cols = train_data.select_dtypes(include=['object']).columns\n",
        "    \n",
        "    if len(categorical_cols) > 0:\n",
        "        print(f\"\\nFound {len(categorical_cols)} categorical columns:\")\n",
        "        for col in categorical_cols:\n",
        "            unique_values = train_data[col].nunique()\n",
        "            print(f\"  {col}: {unique_values} unique values\")\n",
        "            if unique_values <= 10:\n",
        "                print(f\"    Values: {train_data[col].value_counts().head().to_dict()}\")\n",
        "    \n",
        "    # Data quality checks\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"DATA QUALITY CHECKS\")\n",
        "    print(\"=\"*30)\n",
        "    \n",
        "    # Check for duplicates\n",
        "    duplicates = train_data.duplicated().sum()\n",
        "    print(f\"Duplicate rows: {duplicates}\")\n",
        "    \n",
        "    # Check for constant columns\n",
        "    constant_cols = []\n",
        "    for col in train_data.columns:\n",
        "        if train_data[col].nunique() == 1:\n",
        "            constant_cols.append(col)\n",
        "    \n",
        "    if constant_cols:\n",
        "        print(f\"Constant columns (same value for all rows): {constant_cols}\")\n",
        "    else:\n",
        "        print(\"No constant columns found\")\n",
        "    \n",
        "    # Memory usage by column\n",
        "    print(\"\\nTop 10 columns by memory usage:\")\n",
        "    memory_usage = train_data.memory_usage(deep=True).sort_values(ascending=False)\n",
        "    for col, usage in memory_usage.head(10).items():\n",
        "        print(f\"  {col}: {usage / (1024**2):.2f} MB\")\n",
        "        \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"READY FOR FURTHER ANALYSIS!\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Data loaded successfully. You can now proceed with:\")\n",
        "    print(\"1. Feature engineering\")\n",
        "    print(\"2. Model building\")\n",
        "    print(\"3. Cross-validation\")\n",
        "    print(\"4. Predictions\")\n",
        "else:\n",
        "    print(\"No data available for additional analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
