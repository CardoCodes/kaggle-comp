{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Ready to analyze DRW Crypto Market Prediction data...\n"
          ]
        }
      ],
      "source": [
        "# DRW - Crypto Market Prediction Analysis\n",
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
        "import seaborn as sns\n",
        "import os\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options for better output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to analyze DRW Crypto Market Prediction data...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and examine the crypto market data\n",
        "# First, let's identify the main data files\n",
        "data_dir = \"./data/\"\n",
        "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
        "print(f\"Found CSV files: {csv_files}\")\n",
        "\n",
        "# Load the main dataset(s)\n",
        "train_data = None\n",
        "test_data = None\n",
        "\n",
        "# Try to identify train and test files\n",
        "for file in csv_files:\n",
        "    if 'train' in file.lower():\n",
        "        print(f\"\\nLoading training data from: {file}\")\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, file))\n",
        "        print(f\"Training data shape: {train_data.shape}\")\n",
        "    elif 'test' in file.lower():\n",
        "        print(f\"\\nLoading test data from: {file}\")\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, file))\n",
        "        print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "# If no train/test files found, load all CSV files\n",
        "if train_data is None and csv_files:\n",
        "    print(f\"\\nNo train file found. Loading first available CSV file: {csv_files[0]}\")\n",
        "    train_data = pd.read_csv(os.path.join(data_dir, csv_files[0]))\n",
        "    print(f\"Data shape: {train_data.shape}\")\n",
        "\n",
        "# Display basic information about the dataset\n",
        "if train_data is not None:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CRYPTO MARKET DATASET OVERVIEW\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Dataset shape: {train_data.shape}\")\n",
        "    print(f\"Number of rows: {train_data.shape[0]:,}\")\n",
        "    print(f\"Number of columns: {train_data.shape[1]:,}\")\n",
        "    \n",
        "    print(\"\\nColumn names:\")\n",
        "    for i, col in enumerate(train_data.columns):\n",
        "        print(f\"  {i+1:2d}. {col}\")\n",
        "        \n",
        "    print(\"\\nData types:\")\n",
        "    print(train_data.dtypes.value_counts())\n",
        "    \n",
        "    print(\"\\nMemory usage:\")\n",
        "    print(f\"Total memory usage: {train_data.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "else:\n",
        "    print(\"No data loaded. Please check if the data files exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the head of the dataset and examine structure\n",
        "if train_data is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"DATA HEAD - First 5 rows\")\n",
        "    print(\"=\"*60)\n",
        "    display(train_data.head())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DATA TAIL - Last 5 rows\")\n",
        "    print(\"=\"*60)\n",
        "    display(train_data.tail())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DATA SAMPLE - Random 5 rows\")\n",
        "    print(\"=\"*60)\n",
        "    display(train_data.sample(5))\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DETAILED COLUMN INFORMATION\")\n",
        "    print(\"=\"*60)\n",
        "    print(train_data.info())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BASIC STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "    display(train_data.describe())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MISSING VALUES ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    missing_values = train_data.isnull().sum()\n",
        "    missing_percent = (missing_values / len(train_data)) * 100\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Column': missing_values.index,\n",
        "        'Missing_Count': missing_values.values,\n",
        "        'Missing_Percent': missing_percent.values\n",
        "    })\n",
        "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "    \n",
        "    if not missing_df.empty:\n",
        "        print(\"Columns with missing values:\")\n",
        "        display(missing_df)\n",
        "    else:\n",
        "        print(\"âœ“ No missing values found!\")\n",
        "        \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"UNIQUE VALUES PER COLUMN\")\n",
        "    print(\"=\"*60)\n",
        "    unique_counts = train_data.nunique().sort_values(ascending=False)\n",
        "    print(unique_counts)\n",
        "    \n",
        "    # Check for potential datetime columns\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"POTENTIAL DATETIME COLUMNS\")\n",
        "    print(\"=\"*60)\n",
        "    potential_datetime_cols = []\n",
        "    for col in train_data.columns:\n",
        "        if train_data[col].dtype == 'object':\n",
        "            sample_values = train_data[col].dropna().head(3).tolist()\n",
        "            print(f\"{col}: {sample_values}\")\n",
        "            if any(keyword in col.lower() for keyword in ['time', 'date', 'timestamp']):\n",
        "                potential_datetime_cols.append(col)\n",
        "    \n",
        "    if potential_datetime_cols:\n",
        "        print(f\"\\nPotential datetime columns: {potential_datetime_cols}\")\n",
        "    \n",
        "    # Check for potential categorical columns\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"POTENTIAL CATEGORICAL COLUMNS\")\n",
        "    print(\"=\"*60)\n",
        "    categorical_cols = train_data.select_dtypes(include=['object']).columns\n",
        "    \n",
        "    if len(categorical_cols) > 0:\n",
        "        print(f\"Object columns found: {list(categorical_cols)}\")\n",
        "        for col in categorical_cols:\n",
        "            unique_values = train_data[col].nunique()\n",
        "            print(f\"\\n{col}:\")\n",
        "            print(f\"  Unique values: {unique_values}\")\n",
        "            if unique_values <= 20:\n",
        "                print(f\"  Value counts: {train_data[col].value_counts().head(10).to_dict()}\")\n",
        "    else:\n",
        "        print(\"No object/categorical columns found\")\n",
        "        \n",
        "else:\n",
        "    print(\"No data to display. Please check data loading.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ” Data Exploration Complete - Ready for Next Steps!\n",
        "\n",
        "## Summary\n",
        "We have successfully loaded and examined the DRW Crypto Market Prediction dataset. The initial exploration gives us a good understanding of the data structure, types, and quality.\n",
        "\n",
        "## ðŸ“Š Suggested Next Steps\n",
        "\n",
        "Based on the initial data exploration, here are the recommended next steps:\n",
        "\n",
        "### 1. **Time Series Analysis** ðŸ•\n",
        "- Convert timestamp columns to datetime format\n",
        "- Analyze temporal patterns and trends\n",
        "- Check for seasonality in crypto prices\n",
        "- Examine trading volume patterns over time\n",
        "\n",
        "### 2. **Feature Engineering** ðŸ”§\n",
        "- Create technical indicators (RSI, MACD, Bollinger Bands)\n",
        "- Calculate moving averages (SMA, EMA)\n",
        "- Generate price change ratios and volatility measures\n",
        "- Create lag features for time series prediction\n",
        "\n",
        "### 3. **Correlation Analysis** ðŸ“ˆ\n",
        "- Examine correlations between different cryptocurrencies\n",
        "- Analyze relationships between price and volume\n",
        "- Study correlation with market indicators\n",
        "- Identify potential multicollinearity issues\n",
        "\n",
        "### 4. **Visualization & EDA** ðŸ“Š\n",
        "- Plot price trends over time\n",
        "- Create candlestick charts\n",
        "- Volume analysis charts\n",
        "- Distribution plots for key features\n",
        "\n",
        "### 5. **Data Preprocessing** ðŸ§¹\n",
        "- Handle missing values (if any)\n",
        "- Normalize/scale features\n",
        "- Create train/validation/test splits\n",
        "- Address any data quality issues\n",
        "\n",
        "### 6. **Model Selection** ðŸ¤–\n",
        "- Time series models (ARIMA, Prophet)\n",
        "- Machine learning models (Random Forest, XGBoost)\n",
        "- Deep learning models (LSTM, GRU)\n",
        "- Ensemble methods\n",
        "\n",
        "### 7. **Evaluation Strategy** ðŸ“\n",
        "- Define appropriate metrics for crypto prediction\n",
        "- Set up cross-validation for time series\n",
        "- Create backtesting framework\n",
        "- Risk-adjusted performance metrics\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸ’¡ Recommendation**: Start with **Time Series Analysis** to understand the temporal nature of the data, then move to **Feature Engineering** to create predictive features before building models.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
